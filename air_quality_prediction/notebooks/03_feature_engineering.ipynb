{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering for Air Quality Prediction\n",
        "\n",
        "This notebook demonstrates the feature engineering pipeline for creating advanced features from air quality data.\n",
        "\n",
        "## Objectives\n",
        "1. Create temporal features (year, month, day, season, cyclical encoding)\n",
        "2. Generate lag features (previous day values)\n",
        "3. Build rolling averages (3-day, 7-day, 14-day)\n",
        "4. Create ratio features (PM2.5/PM10 ratio)\n",
        "5. Develop interaction features\n",
        "6. Select optimal features for modeling\n",
        "\n",
        "## Feature Categories\n",
        "- **Temporal Features**: Time-based features with cyclical encoding\n",
        "- **Lag Features**: Previous day values for all pollutants\n",
        "- **Rolling Features**: Moving averages and standard deviations\n",
        "- **Ratio Features**: Relationships between pollutants\n",
        "- **Interaction Features**: Combined effects of multiple variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pandas is already installed\n",
            "numpy is already installed\n",
            "matplotlib is already installed\n",
            "seaborn is already installed\n",
            "Installing scikit-learn...\n",
            "scipy is already installed\n",
            "tqdm is already installed\n",
            "joblib is already installed\n",
            "Feature engineering libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package using pip if not already installed.\"\"\"\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"scikit-learn\",\n",
        "    \"scipy\",\n",
        "    \"tqdm\",\n",
        "    \"joblib\"\n",
        "]\n",
        "\n",
        "for package in required_packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Feature engineering libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Feature Engineering Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Loading processed data for feature engineering...\n",
            "Data path: ../data/processed/\n",
            "‚úÖ Loaded processed data successfully!\n",
            "üìä Dataset shape: (7688, 16)\n",
            "üìã Columns: ['City', 'Date', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket']\n",
            "\n",
            "üìà Basic Information:\n",
            "  Memory usage: 2.04 MB\n",
            "  Date range: 2015-01-01 to 2020-07-01\n",
            "  Cities: {'Delhi': np.int64(1999), 'Chennai': np.int64(1884), 'Hyderabad': np.int64(1880), 'Visakhapatnam': np.int64(1171), 'Kolkata': np.int64(754)}\n",
            "  AQI range: 22.00 to 346.50\n",
            "  AQI mean: 148.85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:feature_engineering:Initialized AirQualityFeatureEngineer with data path: ../data/processed/temp_processed_data.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Feature engineer initialized with processed data!\n"
          ]
        }
      ],
      "source": [
        "# Load processed data\n",
        "data_path = \"../data/processed/\"\n",
        "\n",
        "print(\"üîç Loading processed data for feature engineering...\")\n",
        "print(f\"Data path: {data_path}\")\n",
        "\n",
        "# Load the cleaned dataset from preprocessing\n",
        "try:\n",
        "    df_processed = pd.read_csv(data_path + \"cleaned_data.csv\")\n",
        "    print(f\"‚úÖ Loaded processed data successfully!\")\n",
        "    print(f\"üìä Dataset shape: {df_processed.shape}\")\n",
        "    print(f\"üìã Columns: {list(df_processed.columns)}\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nüìà Basic Information:\")\n",
        "    print(f\"  Memory usage: {df_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"  Date range: {df_processed['Date'].min()} to {df_processed['Date'].max()}\")\n",
        "    \n",
        "    # Check cities\n",
        "    if 'City' in df_processed.columns:\n",
        "        city_counts = df_processed['City'].value_counts()\n",
        "        print(f\"  Cities: {dict(city_counts)}\")\n",
        "    \n",
        "    # Check AQI\n",
        "    if 'AQI' in df_processed.columns:\n",
        "        print(f\"  AQI range: {df_processed['AQI'].min():.2f} to {df_processed['AQI'].max():.2f}\")\n",
        "        print(f\"  AQI mean: {df_processed['AQI'].mean():.2f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading processed data: {e}\")\n",
        "    df_processed = pd.DataFrame()\n",
        "\n",
        "# Import feature engineering class\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from feature_engineering import AirQualityFeatureEngineer\n",
        "\n",
        "# Initialize feature engineer\n",
        "if not df_processed.empty:\n",
        "    # Save a temporary copy for the feature engineer\n",
        "    df_processed.to_csv(data_path + \"temp_processed_data.csv\", index=False)\n",
        "    feature_engineer = AirQualityFeatureEngineer(data_path + \"temp_processed_data.csv\")\n",
        "    print(\"‚úÖ Feature engineer initialized with processed data!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Feature engineer initialized with placeholder path\")\n",
        "    feature_engineer = AirQualityFeatureEngineer(data_path + \"cleaned_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Temporal Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÖ CREATING TEMPORAL FEATURES\n",
            "==================================================\n",
            "‚úÖ Created 15 temporal features\n",
            "Temporal features: ['year', 'month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', 'season', 'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos', 'day_of_year_sin', 'day_of_year_cos', 'is_weekend', 'is_month_end']\n",
            "\n",
            "üìä Temporal Feature Summary:\n",
            "  Date range: 2015-01-01 00:00:00 to 2020-07-01 00:00:00\n",
            "  Years: [np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020)]\n",
            "  Seasons: {'Spring': 2089, 'Winter': 1921, 'Summer': 1871, 'Fall': 1807}\n",
            "  Weekend ratio: 0.29\n",
            "\n",
            "üìà Dataset shape after temporal features: (7688, 31)\n"
          ]
        }
      ],
      "source": [
        "# Create temporal features from Date column\n",
        "print(\"üìÖ CREATING TEMPORAL FEATURES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_processed.empty and 'Date' in df_processed.columns:\n",
        "    # Make a copy for feature engineering\n",
        "    df_features = df_processed.copy()\n",
        "    \n",
        "    # Convert Date to datetime if not already\n",
        "    df_features['Date'] = pd.to_datetime(df_features['Date'])\n",
        "    \n",
        "    # Extract basic temporal features\n",
        "    df_features['year'] = df_features['Date'].dt.year\n",
        "    df_features['month'] = df_features['Date'].dt.month\n",
        "    df_features['day'] = df_features['Date'].dt.day\n",
        "    df_features['day_of_week'] = df_features['Date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "    df_features['day_of_year'] = df_features['Date'].dt.dayofyear\n",
        "    df_features['week_of_year'] = df_features['Date'].dt.isocalendar().week\n",
        "    \n",
        "    # Create season feature\n",
        "    season_mapping = {\n",
        "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "        9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
        "    }\n",
        "    df_features['season'] = df_features['month'].map(season_mapping)\n",
        "    \n",
        "    # Create cyclical encoding for temporal features\n",
        "    # This helps the model understand that December is close to January\n",
        "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
        "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
        "    df_features['day_of_week_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\n",
        "    df_features['day_of_week_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\n",
        "    df_features['day_of_year_sin'] = np.sin(2 * np.pi * df_features['day_of_year'] / 365)\n",
        "    df_features['day_of_year_cos'] = np.cos(2 * np.pi * df_features['day_of_year'] / 365)\n",
        "    \n",
        "    # Create weekend indicator\n",
        "    df_features['is_weekend'] = (df_features['day_of_week'] >= 5).astype(int)\n",
        "    \n",
        "    # Create month-end indicator (last 3 days of month)\n",
        "    df_features['is_month_end'] = (df_features['day'] >= 29).astype(int)\n",
        "    \n",
        "    temporal_features = [\n",
        "        'year', 'month', 'day', 'day_of_week', 'day_of_year', 'week_of_year',\n",
        "        'season', 'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
        "        'day_of_year_sin', 'day_of_year_cos', 'is_weekend', 'is_month_end'\n",
        "    ]\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(temporal_features)} temporal features\")\n",
        "    print(f\"Temporal features: {temporal_features}\")\n",
        "    \n",
        "    # Display temporal feature summary\n",
        "    print(f\"\\nüìä Temporal Feature Summary:\")\n",
        "    print(f\"  Date range: {df_features['Date'].min()} to {df_features['Date'].max()}\")\n",
        "    print(f\"  Years: {sorted(df_features['year'].unique())}\")\n",
        "    print(f\"  Seasons: {df_features['season'].value_counts().to_dict()}\")\n",
        "    print(f\"  Weekend ratio: {df_features['is_weekend'].mean():.2f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot create temporal features - Date column not found\")\n",
        "    df_features = df_processed.copy()\n",
        "    temporal_features = []\n",
        "\n",
        "print(f\"\\nüìà Dataset shape after temporal features: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Lag Features (Previous Day Values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∞ CREATING LAG FEATURES\n",
            "==================================================\n",
            "Creating lag features for 11 pollutants:\n",
            "Pollutants: ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene']\n",
            "Lag days: [1, 2, 3, 7]\n",
            "  ‚úÖ prev_1d_PM2.5\n",
            "  ‚úÖ prev_2d_PM2.5\n",
            "  ‚úÖ prev_3d_PM2.5\n",
            "  ‚úÖ prev_7d_PM2.5\n",
            "  ‚úÖ prev_1d_PM10\n",
            "  ‚úÖ prev_2d_PM10\n",
            "  ‚úÖ prev_3d_PM10\n",
            "  ‚úÖ prev_7d_PM10\n",
            "  ‚úÖ prev_1d_NO\n",
            "  ‚úÖ prev_2d_NO\n",
            "  ‚úÖ prev_3d_NO\n",
            "  ‚úÖ prev_7d_NO\n",
            "  ‚úÖ prev_1d_NO2\n",
            "  ‚úÖ prev_2d_NO2\n",
            "  ‚úÖ prev_3d_NO2\n",
            "  ‚úÖ prev_7d_NO2\n",
            "  ‚úÖ prev_1d_NOx\n",
            "  ‚úÖ prev_2d_NOx\n",
            "  ‚úÖ prev_3d_NOx\n",
            "  ‚úÖ prev_7d_NOx\n",
            "  ‚úÖ prev_1d_NH3\n",
            "  ‚úÖ prev_2d_NH3\n",
            "  ‚úÖ prev_3d_NH3\n",
            "  ‚úÖ prev_7d_NH3\n",
            "  ‚úÖ prev_1d_CO\n",
            "  ‚úÖ prev_2d_CO\n",
            "  ‚úÖ prev_3d_CO\n",
            "  ‚úÖ prev_7d_CO\n",
            "  ‚úÖ prev_1d_SO2\n",
            "  ‚úÖ prev_2d_SO2\n",
            "  ‚úÖ prev_3d_SO2\n",
            "  ‚úÖ prev_7d_SO2\n",
            "  ‚úÖ prev_1d_O3\n",
            "  ‚úÖ prev_2d_O3\n",
            "  ‚úÖ prev_3d_O3\n",
            "  ‚úÖ prev_7d_O3\n",
            "  ‚úÖ prev_1d_Benzene\n",
            "  ‚úÖ prev_2d_Benzene\n",
            "  ‚úÖ prev_3d_Benzene\n",
            "  ‚úÖ prev_7d_Benzene\n",
            "  ‚úÖ prev_1d_Toluene\n",
            "  ‚úÖ prev_2d_Toluene\n",
            "  ‚úÖ prev_3d_Toluene\n",
            "  ‚úÖ prev_7d_Toluene\n",
            "  ‚úÖ prev_1d_AQI\n",
            "  ‚úÖ prev_2d_AQI\n",
            "  ‚úÖ prev_3d_AQI\n",
            "  ‚úÖ prev_7d_AQI\n",
            "\n",
            "‚úÖ Created 48 lag features\n",
            "\n",
            "‚ö†Ô∏è Missing values in lag features (first few days for each city):\n",
            "  prev_1d_PM2.5: 25 missing values\n",
            "  prev_2d_PM2.5: 30 missing values\n",
            "  prev_3d_PM2.5: 35 missing values\n",
            "  prev_7d_PM2.5: 55 missing values\n",
            "  prev_1d_PM10: 1914 missing values\n",
            "  prev_2d_PM10: 1919 missing values\n",
            "  prev_3d_PM10: 1924 missing values\n",
            "  prev_7d_PM10: 1944 missing values\n",
            "  prev_1d_NO: 23 missing values\n",
            "  prev_2d_NO: 28 missing values\n",
            "\n",
            "üìä Lag Feature Summary:\n",
            "  Total lag features: 48\n",
            "  Pollutants with lag features: 11\n",
            "  Lag days: [1, 2, 3, 7]\n",
            "\n",
            "üìà Dataset shape after lag features: (7688, 79)\n"
          ]
        }
      ],
      "source": [
        "# Create lag features for pollutants and AQI\n",
        "print(\"‚è∞ CREATING LAG FEATURES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_features.empty and 'City' in df_features.columns:\n",
        "    # Sort data by city and date for proper lag calculation\n",
        "    df_features = df_features.sort_values(['City', 'Date']).reset_index(drop=True)\n",
        "    \n",
        "    # Define pollutant columns (excluding target variable AQI for now)\n",
        "    pollutant_columns = [col for col in df_features.columns \n",
        "                        if col in ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene']]\n",
        "    \n",
        "    # Define lag days to create\n",
        "    lag_days = [1, 2, 3, 7]  # Previous 1, 2, 3, and 7 days\n",
        "    \n",
        "    print(f\"Creating lag features for {len(pollutant_columns)} pollutants:\")\n",
        "    print(f\"Pollutants: {pollutant_columns}\")\n",
        "    print(f\"Lag days: {lag_days}\")\n",
        "    \n",
        "    lag_features = []\n",
        "    \n",
        "    # Create lag features for pollutants\n",
        "    for pollutant in pollutant_columns:\n",
        "        if pollutant in df_features.columns:\n",
        "            for lag in lag_days:\n",
        "                lag_col_name = f'prev_{lag}d_{pollutant}'\n",
        "                df_features[lag_col_name] = df_features.groupby('City')[pollutant].shift(lag)\n",
        "                lag_features.append(lag_col_name)\n",
        "                print(f\"  ‚úÖ {lag_col_name}\")\n",
        "    \n",
        "    # Create lag features for AQI (target variable)\n",
        "    if 'AQI' in df_features.columns:\n",
        "        for lag in lag_days:\n",
        "            lag_col_name = f'prev_{lag}d_AQI'\n",
        "            df_features[lag_col_name] = df_features.groupby('City')['AQI'].shift(lag)\n",
        "            lag_features.append(lag_col_name)\n",
        "            print(f\"  ‚úÖ {lag_col_name}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Created {len(lag_features)} lag features\")\n",
        "    \n",
        "    # Check for missing values in lag features\n",
        "    lag_missing = df_features[lag_features].isnull().sum()\n",
        "    lag_missing = lag_missing[lag_missing > 0]\n",
        "    \n",
        "    if len(lag_missing) > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è Missing values in lag features (first few days for each city):\")\n",
        "        for col, count in lag_missing.head(10).items():\n",
        "            print(f\"  {col}: {count} missing values\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ No missing values in lag features\")\n",
        "    \n",
        "    # Display lag feature summary\n",
        "    print(f\"\\nüìä Lag Feature Summary:\")\n",
        "    print(f\"  Total lag features: {len(lag_features)}\")\n",
        "    print(f\"  Pollutants with lag features: {len(pollutant_columns)}\")\n",
        "    print(f\"  Lag days: {lag_days}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot create lag features - City column not found\")\n",
        "    lag_features = []\n",
        "\n",
        "print(f\"\\nüìà Dataset shape after lag features: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Rolling Features (Moving Averages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà CREATING ROLLING FEATURES\n",
            "==================================================\n",
            "Creating rolling features with windows: [3, 7, 14]\n",
            "  ‚úÖ PM2.5: 3d avg, std, max\n",
            "  ‚úÖ PM2.5: 7d avg, std, max\n",
            "  ‚úÖ PM2.5: 14d avg, std, max\n",
            "  ‚úÖ PM10: 3d avg, std, max\n",
            "  ‚úÖ PM10: 7d avg, std, max\n",
            "  ‚úÖ PM10: 14d avg, std, max\n",
            "  ‚úÖ NO: 3d avg, std, max\n",
            "  ‚úÖ NO: 7d avg, std, max\n",
            "  ‚úÖ NO: 14d avg, std, max\n",
            "  ‚úÖ NO2: 3d avg, std, max\n",
            "  ‚úÖ NO2: 7d avg, std, max\n",
            "  ‚úÖ NO2: 14d avg, std, max\n",
            "  ‚úÖ NOx: 3d avg, std, max\n",
            "  ‚úÖ NOx: 7d avg, std, max\n",
            "  ‚úÖ NOx: 14d avg, std, max\n",
            "  ‚úÖ NH3: 3d avg, std, max\n",
            "  ‚úÖ NH3: 7d avg, std, max\n",
            "  ‚úÖ NH3: 14d avg, std, max\n",
            "  ‚úÖ CO: 3d avg, std, max\n",
            "  ‚úÖ CO: 7d avg, std, max\n",
            "  ‚úÖ CO: 14d avg, std, max\n",
            "  ‚úÖ SO2: 3d avg, std, max\n",
            "  ‚úÖ SO2: 7d avg, std, max\n",
            "  ‚úÖ SO2: 14d avg, std, max\n",
            "  ‚úÖ O3: 3d avg, std, max\n",
            "  ‚úÖ O3: 7d avg, std, max\n",
            "  ‚úÖ O3: 14d avg, std, max\n",
            "  ‚úÖ Benzene: 3d avg, std, max\n",
            "  ‚úÖ Benzene: 7d avg, std, max\n",
            "  ‚úÖ Benzene: 14d avg, std, max\n",
            "  ‚úÖ Toluene: 3d avg, std, max\n",
            "  ‚úÖ Toluene: 7d avg, std, max\n",
            "  ‚úÖ Toluene: 14d avg, std, max\n",
            "  ‚úÖ AQI: 3d average\n",
            "  ‚úÖ AQI: 7d average\n",
            "  ‚úÖ AQI: 14d average\n",
            "\n",
            "‚úÖ Created 102 rolling features\n",
            "\n",
            "üìä Rolling Feature Summary:\n",
            "  Total rolling features: 102\n",
            "  Rolling windows: [3, 7, 14]\n",
            "  Statistics per window: mean, std, max\n",
            "\n",
            "‚ö†Ô∏è Missing values in rolling features:\n",
            "  3d_std_PM2.5: 5 missing values\n",
            "  7d_std_PM2.5: 5 missing values\n",
            "  14d_std_PM2.5: 5 missing values\n",
            "  3d_avg_PM10: 1862 missing values\n",
            "  3d_std_PM10: 1897 missing values\n",
            "  3d_max_PM10: 1862 missing values\n",
            "  7d_avg_PM10: 1831 missing values\n",
            "  7d_std_PM10: 1853 missing values\n",
            "  7d_max_PM10: 1831 missing values\n",
            "  14d_avg_PM10: 1793 missing values\n",
            "\n",
            "üìà Dataset shape after rolling features: (7688, 181)\n"
          ]
        }
      ],
      "source": [
        "# Create rolling average and standard deviation features\n",
        "print(\"üìà CREATING ROLLING FEATURES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_features.empty and 'City' in df_features.columns:\n",
        "    # Define rolling windows\n",
        "    rolling_windows = [3, 7, 14]  # 3-day, 7-day, and 14-day rolling windows\n",
        "    \n",
        "    print(f\"Creating rolling features with windows: {rolling_windows}\")\n",
        "    \n",
        "    rolling_features = []\n",
        "    \n",
        "    # Create rolling features for pollutants\n",
        "    for pollutant in pollutant_columns:\n",
        "        if pollutant in df_features.columns:\n",
        "            for window in rolling_windows:\n",
        "                # Rolling mean\n",
        "                mean_col = f'{window}d_avg_{pollutant}'\n",
        "                df_features[mean_col] = df_features.groupby('City')[pollutant].rolling(\n",
        "                    window=window, min_periods=1\n",
        "                ).mean().reset_index(0, drop=True)\n",
        "                rolling_features.append(mean_col)\n",
        "                \n",
        "                # Rolling standard deviation\n",
        "                std_col = f'{window}d_std_{pollutant}'\n",
        "                df_features[std_col] = df_features.groupby('City')[pollutant].rolling(\n",
        "                    window=window, min_periods=1\n",
        "                ).std().reset_index(0, drop=True)\n",
        "                rolling_features.append(std_col)\n",
        "                \n",
        "                # Rolling maximum\n",
        "                max_col = f'{window}d_max_{pollutant}'\n",
        "                df_features[max_col] = df_features.groupby('City')[pollutant].rolling(\n",
        "                    window=window, min_periods=1\n",
        "                ).max().reset_index(0, drop=True)\n",
        "                rolling_features.append(max_col)\n",
        "                \n",
        "                print(f\"  ‚úÖ {pollutant}: {window}d avg, std, max\")\n",
        "    \n",
        "    # Create rolling features for AQI\n",
        "    if 'AQI' in df_features.columns:\n",
        "        for window in rolling_windows:\n",
        "            # Rolling mean for AQI\n",
        "            aqi_mean_col = f'{window}d_avg_AQI'\n",
        "            df_features[aqi_mean_col] = df_features.groupby('City')['AQI'].rolling(\n",
        "                window=window, min_periods=1\n",
        "            ).mean().reset_index(0, drop=True)\n",
        "            rolling_features.append(aqi_mean_col)\n",
        "            \n",
        "            print(f\"  ‚úÖ AQI: {window}d average\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Created {len(rolling_features)} rolling features\")\n",
        "    \n",
        "    # Display rolling feature summary\n",
        "    print(f\"\\nüìä Rolling Feature Summary:\")\n",
        "    print(f\"  Total rolling features: {len(rolling_features)}\")\n",
        "    print(f\"  Rolling windows: {rolling_windows}\")\n",
        "    print(f\"  Statistics per window: mean, std, max\")\n",
        "    \n",
        "    # Check for missing values in rolling features\n",
        "    rolling_missing = df_features[rolling_features].isnull().sum()\n",
        "    rolling_missing = rolling_missing[rolling_missing > 0]\n",
        "    \n",
        "    if len(rolling_missing) > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è Missing values in rolling features:\")\n",
        "        for col, count in rolling_missing.head(10).items():\n",
        "            print(f\"  {col}: {count} missing values\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ No missing values in rolling features\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot create rolling features - City column not found\")\n",
        "    rolling_features = []\n",
        "\n",
        "print(f\"\\nüìà Dataset shape after rolling features: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Ratio Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó CREATING RATIO FEATURES\n",
            "==================================================\n",
            "‚úÖ PM25_PM10_ratio - Fine vs coarse particle ratio\n",
            "‚úÖ NO2_NO_ratio - Atmospheric oxidation indicator\n",
            "‚úÖ SO2_NO2_ratio - Pollution source type indicator\n",
            "‚úÖ CO_PM25_ratio - Combustion vs particulate pollution\n",
            "‚úÖ O3_NO2_ratio - Photochemical smog indicator\n",
            "‚úÖ Benzene_Toluene_ratio - Aromatic compound source\n",
            "‚úÖ NH3_NOx_ratio - Agricultural vs traffic pollution\n",
            "\n",
            "‚úÖ Created 7 ratio features\n",
            "Ratio features: ['PM25_PM10_ratio', 'NO2_NO_ratio', 'SO2_NO2_ratio', 'CO_PM25_ratio', 'O3_NO2_ratio', 'Benzene_Toluene_ratio', 'NH3_NOx_ratio']\n",
            "\n",
            "üìä Ratio Feature Statistics:\n",
            "  PM25_PM10_ratio:\n",
            "    Range: 0.051 to 151.952\n",
            "    Mean: 0.534, Std: 2.134\n",
            "  NO2_NO_ratio:\n",
            "    Range: 0.061 to 67.125\n",
            "    Mean: 3.270, Std: 2.942\n",
            "  SO2_NO2_ratio:\n",
            "    Range: 0.022 to 25.647\n",
            "    Mean: 0.424, Std: 0.690\n",
            "  CO_PM25_ratio:\n",
            "    Range: 0.000 to 0.451\n",
            "    Mean: 0.019, Std: 0.016\n",
            "  O3_NO2_ratio:\n",
            "    Range: 0.033 to 473.118\n",
            "    Mean: 1.755, Std: 6.630\n",
            "  Benzene_Toluene_ratio:\n",
            "    Range: 0.000 to 166.333\n",
            "    Mean: 0.701, Std: 4.562\n",
            "  NH3_NOx_ratio:\n",
            "    Range: 0.000 to 29.599\n",
            "    Mean: 1.352, Std: 1.741\n",
            "\n",
            "üìà Dataset shape after ratio features: (7688, 188)\n"
          ]
        }
      ],
      "source": [
        "# Create ratio features between different pollutants\n",
        "print(\"üîó CREATING RATIO FEATURES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ratio_features = []\n",
        "\n",
        "# PM2.5/PM10 ratio - indicates fine vs coarse particles\n",
        "if 'PM2.5' in df_features.columns and 'PM10' in df_features.columns:\n",
        "    df_features['PM25_PM10_ratio'] = np.where(\n",
        "        df_features['PM10'] != 0, \n",
        "        df_features['PM2.5'] / df_features['PM10'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('PM25_PM10_ratio')\n",
        "    print(\"‚úÖ PM25_PM10_ratio - Fine vs coarse particle ratio\")\n",
        "\n",
        "# NO2/NO ratio - indicates atmospheric oxidation\n",
        "if 'NO2' in df_features.columns and 'NO' in df_features.columns:\n",
        "    df_features['NO2_NO_ratio'] = np.where(\n",
        "        df_features['NO'] != 0, \n",
        "        df_features['NO2'] / df_features['NO'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('NO2_NO_ratio')\n",
        "    print(\"‚úÖ NO2_NO_ratio - Atmospheric oxidation indicator\")\n",
        "\n",
        "# SO2/NO2 ratio - indicates pollution source type\n",
        "if 'SO2' in df_features.columns and 'NO2' in df_features.columns:\n",
        "    df_features['SO2_NO2_ratio'] = np.where(\n",
        "        df_features['NO2'] != 0, \n",
        "        df_features['SO2'] / df_features['NO2'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('SO2_NO2_ratio')\n",
        "    print(\"‚úÖ SO2_NO2_ratio - Pollution source type indicator\")\n",
        "\n",
        "# CO/PM2.5 ratio - indicates combustion vs particulate pollution\n",
        "if 'CO' in df_features.columns and 'PM2.5' in df_features.columns:\n",
        "    df_features['CO_PM25_ratio'] = np.where(\n",
        "        df_features['PM2.5'] != 0, \n",
        "        df_features['CO'] / df_features['PM2.5'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('CO_PM25_ratio')\n",
        "    print(\"‚úÖ CO_PM25_ratio - Combustion vs particulate pollution\")\n",
        "\n",
        "# O3/NO2 ratio - indicates photochemical smog\n",
        "if 'O3' in df_features.columns and 'NO2' in df_features.columns:\n",
        "    df_features['O3_NO2_ratio'] = np.where(\n",
        "        df_features['NO2'] != 0, \n",
        "        df_features['O3'] / df_features['NO2'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('O3_NO2_ratio')\n",
        "    print(\"‚úÖ O3_NO2_ratio - Photochemical smog indicator\")\n",
        "\n",
        "# Benzene/Toluene ratio - indicates aromatic compound source\n",
        "if 'Benzene' in df_features.columns and 'Toluene' in df_features.columns:\n",
        "    df_features['Benzene_Toluene_ratio'] = np.where(\n",
        "        df_features['Toluene'] != 0, \n",
        "        df_features['Benzene'] / df_features['Toluene'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('Benzene_Toluene_ratio')\n",
        "    print(\"‚úÖ Benzene_Toluene_ratio - Aromatic compound source\")\n",
        "\n",
        "# NH3/NOx ratio - indicates agricultural vs traffic pollution\n",
        "if 'NH3' in df_features.columns and 'NOx' in df_features.columns:\n",
        "    df_features['NH3_NOx_ratio'] = np.where(\n",
        "        df_features['NOx'] != 0, \n",
        "        df_features['NH3'] / df_features['NOx'], \n",
        "        0\n",
        "    )\n",
        "    ratio_features.append('NH3_NOx_ratio')\n",
        "    print(\"‚úÖ NH3_NOx_ratio - Agricultural vs traffic pollution\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(ratio_features)} ratio features\")\n",
        "print(f\"Ratio features: {ratio_features}\")\n",
        "\n",
        "# Display ratio feature statistics\n",
        "print(f\"\\nüìä Ratio Feature Statistics:\")\n",
        "for ratio_feature in ratio_features:\n",
        "    if ratio_feature in df_features.columns:\n",
        "        stats = df_features[ratio_feature].describe()\n",
        "        print(f\"  {ratio_feature}:\")\n",
        "        print(f\"    Range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
        "        print(f\"    Mean: {stats['mean']:.3f}, Std: {stats['std']:.3f}\")\n",
        "\n",
        "print(f\"\\nüìà Dataset shape after ratio features: {df_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Summary and Save Engineered Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä FEATURE ENGINEERING SUMMARY\n",
            "============================================================\n",
            "üìà Feature Engineering Results:\n",
            "  Original features: 16\n",
            "  Engineered features: 172\n",
            "  Total features: 188\n",
            "  Feature increase: 172\n",
            "\n",
            "üîß Feature Categories:\n",
            "  üìÖ Temporal features: 15\n",
            "  ‚è∞ Lag features: 48\n",
            "  üìà Rolling features: 102\n",
            "  üîó Ratio features: 7\n",
            "\n",
            "üìä Dataset Information:\n",
            "  Shape: (7688, 188)\n",
            "  Memory usage: 11.92 MB\n",
            "  Date range: 2015-01-01 00:00:00 to 2020-07-01 00:00:00\n",
            "\n",
            "‚ö†Ô∏è Missing values in engineered features:\n",
            "  prev_1d_PM2.5: 25 (0.33%)\n",
            "  prev_2d_PM2.5: 30 (0.39%)\n",
            "  prev_3d_PM2.5: 35 (0.46%)\n",
            "  prev_7d_PM2.5: 55 (0.72%)\n",
            "  prev_1d_PM10: 1,914 (24.90%)\n",
            "  prev_2d_PM10: 1,919 (24.96%)\n",
            "  prev_3d_PM10: 1,924 (25.03%)\n",
            "  prev_7d_PM10: 1,944 (25.29%)\n",
            "  prev_1d_NO: 23 (0.30%)\n",
            "  prev_2d_NO: 28 (0.36%)\n",
            "\n",
            "üíæ SAVING ENGINEERED DATASET\n",
            "==================================================\n",
            "‚úÖ Engineered dataset saved to: ../data/features/engineered_features.csv\n",
            "   Shape: (7688, 188)\n",
            "   File size: 14.75 MB\n",
            "‚úÖ Feature metadata saved to: ../data/features/feature_metadata.txt\n",
            "\n",
            "üéâ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
            "üìà Dataset transformed from 16 to 188 features\n",
            "üöÄ Ready for model training! Next step: 04_model_training.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Feature engineering summary and save dataset\n",
        "print(\"üìä FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate total engineered features\n",
        "all_engineered_features = temporal_features + lag_features + rolling_features + ratio_features\n",
        "total_engineered = len(all_engineered_features)\n",
        "\n",
        "print(f\"üìà Feature Engineering Results:\")\n",
        "print(f\"  Original features: {len(df_processed.columns)}\")\n",
        "print(f\"  Engineered features: {total_engineered}\")\n",
        "print(f\"  Total features: {len(df_features.columns)}\")\n",
        "print(f\"  Feature increase: {len(df_features.columns) - len(df_processed.columns)}\")\n",
        "\n",
        "print(f\"\\nüîß Feature Categories:\")\n",
        "print(f\"  üìÖ Temporal features: {len(temporal_features)}\")\n",
        "print(f\"  ‚è∞ Lag features: {len(lag_features)}\")\n",
        "print(f\"  üìà Rolling features: {len(rolling_features)}\")\n",
        "print(f\"  üîó Ratio features: {len(ratio_features)}\")\n",
        "\n",
        "print(f\"\\nüìä Dataset Information:\")\n",
        "print(f\"  Shape: {df_features.shape}\")\n",
        "print(f\"  Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"  Date range: {df_features['Date'].min()} to {df_features['Date'].max()}\")\n",
        "\n",
        "# Check for missing values in engineered features\n",
        "engineered_missing = df_features[all_engineered_features].isnull().sum()\n",
        "engineered_missing = engineered_missing[engineered_missing > 0]\n",
        "\n",
        "if len(engineered_missing) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è Missing values in engineered features:\")\n",
        "    for col, count in engineered_missing.head(10).items():\n",
        "        percentage = (count / len(df_features)) * 100\n",
        "        print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No missing values in engineered features\")\n",
        "\n",
        "# Save engineered dataset\n",
        "print(f\"\\nüíæ SAVING ENGINEERED DATASET\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"../data/features/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save the engineered dataset\n",
        "output_file = output_dir + \"engineered_features.csv\"\n",
        "df_features.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"‚úÖ Engineered dataset saved to: {output_file}\")\n",
        "print(f\"   Shape: {df_features.shape}\")\n",
        "print(f\"   File size: {os.path.getsize(output_file) / 1024**2:.2f} MB\")\n",
        "\n",
        "# Save feature metadata\n",
        "metadata = {\n",
        "    'temporal_features': temporal_features,\n",
        "    'lag_features': lag_features,\n",
        "    'rolling_features': rolling_features,\n",
        "    'ratio_features': ratio_features,\n",
        "    'total_engineered_features': total_engineered,\n",
        "    'dataset_shape': df_features.shape,\n",
        "    'date_range': f\"{df_features['Date'].min()} to {df_features['Date'].max()}\"\n",
        "}\n",
        "\n",
        "# Save metadata as a summary\n",
        "metadata_file = output_dir + \"feature_metadata.txt\"\n",
        "with open(metadata_file, 'w') as f:\n",
        "    f.write(\"FEATURE ENGINEERING METADATA\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    f.write(f\"Total Engineered Features: {total_engineered}\\n\")\n",
        "    f.write(f\"Dataset Shape: {df_features.shape}\\n\")\n",
        "    f.write(f\"Date Range: {metadata['date_range']}\\n\\n\")\n",
        "    \n",
        "    f.write(\"TEMPORAL FEATURES:\\n\")\n",
        "    for feature in temporal_features:\n",
        "        f.write(f\"  - {feature}\\n\")\n",
        "    \n",
        "    f.write(f\"\\nLAG FEATURES ({len(lag_features)}):\\n\")\n",
        "    for feature in lag_features:\n",
        "        f.write(f\"  - {feature}\\n\")\n",
        "    \n",
        "    f.write(f\"\\nROLLING FEATURES ({len(rolling_features)}):\\n\")\n",
        "    for feature in rolling_features:\n",
        "        f.write(f\"  - {feature}\\n\")\n",
        "    \n",
        "    f.write(f\"\\nRATIO FEATURES ({len(ratio_features)}):\\n\")\n",
        "    for feature in ratio_features:\n",
        "        f.write(f\"  - {feature}\\n\")\n",
        "\n",
        "print(f\"‚úÖ Feature metadata saved to: {metadata_file}\")\n",
        "\n",
        "print(f\"\\nüéâ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"üìà Dataset transformed from {len(df_processed.columns)} to {len(df_features.columns)} features\")\n",
        "print(f\"üöÄ Ready for model training! Next step: 04_model_training.ipynb\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PredictiveAnalysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
