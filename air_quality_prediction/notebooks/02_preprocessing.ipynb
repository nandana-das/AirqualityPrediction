{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing for Air Quality Prediction\n",
        "\n",
        "This notebook demonstrates the data preprocessing pipeline for the air quality prediction project.\n",
        "\n",
        "## Objectives\n",
        "1. Load raw air quality data\n",
        "2. Handle missing values and outliers\n",
        "3. Filter data for target cities\n",
        "4. Apply data cleaning techniques\n",
        "5. Prepare data for feature engineering\n",
        "\n",
        "## Preprocessing Steps\n",
        "- Remove rows with null AQI values\n",
        "- Treat outliers using IQR and Z-score methods\n",
        "- Apply SMOTE for data balancing\n",
        "- Scale numerical features\n",
        "- Save processed data for next steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pandas is already installed\n",
            "numpy is already installed\n",
            "matplotlib is already installed\n",
            "seaborn is already installed\n",
            "Installing scikit-learn...\n",
            "Installing imbalanced-learn...\n",
            "tqdm is already installed\n",
            "joblib is already installed\n",
            "Preprocessing libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package using pip if not already installed.\"\"\"\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"scikit-learn\",\n",
        "    \"imbalanced-learn\",\n",
        "    \"tqdm\",\n",
        "    \"joblib\"\n",
        "]\n",
        "\n",
        "for package in required_packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Preprocessing libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking available dataset files...\n",
            "Available files: ['city_day.csv', 'city_hour.csv', 'stations.csv', 'station_day.csv', 'station_hour.csv']\n",
            "‚ö†Ô∏è city_day file not found. Using first available file...\n",
            "‚úÖ Loaded city_day.csv as main dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:data_preprocessing:Initialized AirQualityPreprocessor with data path: ../data/raw/temp_raw_data.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocessor initialized with actual dataset!\n"
          ]
        }
      ],
      "source": [
        "# Load raw data\n",
        "data_path = \"../data/raw/\"\n",
        "\n",
        "# Check available files and load the dataset\n",
        "print(\"üîç Checking available dataset files...\")\n",
        "if os.path.exists(data_path):\n",
        "    files = os.listdir(data_path)\n",
        "    print(f\"Available files: {files}\")\n",
        "    \n",
        "    # Load the main dataset - city_day (daily city-level data)\n",
        "    try:\n",
        "        # Try loading city_day file (main dataset for our project)\n",
        "        if 'city_day' in files:\n",
        "            # Check if it's CSV or Excel format\n",
        "            if 'city_day.csv' in files:\n",
        "                df_raw = pd.read_csv(data_path + 'city_day.csv')\n",
        "                print(\"‚úÖ Loaded city_day.csv successfully!\")\n",
        "            else:\n",
        "                # Try as Excel file\n",
        "                df_raw = pd.read_excel(data_path + 'city_day')\n",
        "                print(\"‚úÖ Loaded city_day Excel file successfully!\")\n",
        "            \n",
        "            print(f\"üìä Raw dataset shape: {df_raw.shape}\")\n",
        "            print(f\"üìã Columns: {list(df_raw.columns)}\")\n",
        "            \n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è city_day file not found. Using first available file...\")\n",
        "            available_files = [f for f in files if f.endswith(('.csv', '.xlsx', '.xls')) or '.' not in f]\n",
        "            if available_files:\n",
        "                first_file = available_files[0]\n",
        "                if first_file.endswith('.csv'):\n",
        "                    df_raw = pd.read_csv(data_path + first_file)\n",
        "                else:\n",
        "                    df_raw = pd.read_excel(data_path + first_file)\n",
        "                print(f\"‚úÖ Loaded {first_file} as main dataset\")\n",
        "            else:\n",
        "                print(\"‚ùå No suitable data files found\")\n",
        "                df_raw = pd.DataFrame()\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        df_raw = pd.DataFrame()\n",
        "        \n",
        "else:\n",
        "    print(f\"‚ùå Data directory not found: {data_path}\")\n",
        "    df_raw = pd.DataFrame()\n",
        "\n",
        "# Import preprocessing class\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from data_preprocessing import AirQualityPreprocessor\n",
        "\n",
        "# Initialize preprocessor with actual data path\n",
        "if not df_raw.empty:\n",
        "    # Save a copy for the preprocessor to use\n",
        "    df_raw.to_csv(data_path + \"temp_raw_data.csv\", index=False)\n",
        "    preprocessor = AirQualityPreprocessor(data_path + \"temp_raw_data.csv\")\n",
        "    print(\"‚úÖ Preprocessor initialized with actual dataset!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Preprocessor initialized with placeholder path\")\n",
        "    preprocessor = AirQualityPreprocessor(data_path + \"air_quality_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Exploration and Quality Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä RAW DATASET ANALYSIS\n",
            "============================================================\n",
            "Dataset Shape: (29531, 16)\n",
            "Memory Usage: 7.71 MB\n",
            "\n",
            "üìã First 5 rows:\n",
            "        City        Date  PM2.5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
            "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
            "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
            "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
            "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
            "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
            "\n",
            "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
            "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
            "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
            "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
            "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
            "4   39.31     7.01    18.89    2.78  NaN        NaN  \n",
            "\n",
            "üìù Data Types and Missing Values:\n",
            "           Data Type  Missing Values  Missing %\n",
            "City          object               0       0.00\n",
            "Date          object               0       0.00\n",
            "PM2.5        float64            4598      15.57\n",
            "PM10         float64           11140      37.72\n",
            "NO           float64            3582      12.13\n",
            "NO2          float64            3585      12.14\n",
            "NOx          float64            4185      14.17\n",
            "NH3          float64           10328      34.97\n",
            "CO           float64            2059       6.97\n",
            "SO2          float64            3854      13.05\n",
            "O3           float64            4022      13.62\n",
            "Benzene      float64            5623      19.04\n",
            "Toluene      float64            8041      27.23\n",
            "Xylene       float64           18109      61.32\n",
            "AQI          float64            4681      15.85\n",
            "AQI_Bucket    object            4681      15.85\n",
            "\n",
            "üèôÔ∏è Total Cities in Dataset: 26\n",
            "First 10 cities: ['Ahmedabad' 'Aizawl' 'Amaravati' 'Amritsar' 'Bengaluru' 'Bhopal'\n",
            " 'Brajrajnagar' 'Chandigarh' 'Chennai' 'Coimbatore']\n",
            "\n",
            "üéØ Target Cities Found (5/6):\n",
            "  ‚úÖ Delhi: 2,009 records\n",
            "  ‚ùå Bangalore: Not found\n",
            "  ‚úÖ Kolkata: 814 records\n",
            "  ‚úÖ Hyderabad: 2,006 records\n",
            "  ‚úÖ Chennai: 2,009 records\n",
            "  ‚úÖ Visakhapatnam: 1,462 records\n",
            "\n",
            "üîç Checking for similar city names...\n",
            "\n",
            "üå¨Ô∏è AQI Analysis:\n",
            "  Range: 13.00 to 2049.00\n",
            "  Mean: 166.46\n",
            "  Median: 118.00\n",
            "  Missing AQI values: 4,681 (15.85%)\n",
            "\n",
            "üìä AQI Distribution:\n",
            "count    24850.000000\n",
            "mean       166.463581\n",
            "std        140.696585\n",
            "min         13.000000\n",
            "25%         81.000000\n",
            "50%        118.000000\n",
            "75%        208.000000\n",
            "max       2049.000000\n",
            "Name: AQI, dtype: float64\n",
            "\n",
            "üìÖ Date Range:\n",
            "  From: 2015-01-01 00:00:00\n",
            "  To: 2020-07-01 00:00:00\n",
            "  Duration: 2008 days\n"
          ]
        }
      ],
      "source": [
        "# Explore the loaded dataset\n",
        "if not df_raw.empty:\n",
        "    print(\"üìä RAW DATASET ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Basic information\n",
        "    print(f\"Dataset Shape: {df_raw.shape}\")\n",
        "    print(f\"Memory Usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(\"\\nüìã First 5 rows:\")\n",
        "    print(df_raw.head())\n",
        "    \n",
        "    # Data types and missing values\n",
        "    print(\"\\nüìù Data Types and Missing Values:\")\n",
        "    missing_info = pd.DataFrame({\n",
        "        'Data Type': df_raw.dtypes,\n",
        "        'Missing Values': df_raw.isnull().sum(),\n",
        "        'Missing %': (df_raw.isnull().sum() / len(df_raw) * 100).round(2)\n",
        "    })\n",
        "    print(missing_info)\n",
        "    \n",
        "    # Check for target cities\n",
        "    target_cities = ['Delhi', 'Bangalore', 'Kolkata', 'Hyderabad', 'Chennai', 'Visakhapatnam']\n",
        "    \n",
        "    if 'City' in df_raw.columns:\n",
        "        available_cities = df_raw['City'].unique()\n",
        "        print(f\"\\nüèôÔ∏è Total Cities in Dataset: {len(available_cities)}\")\n",
        "        print(f\"First 10 cities: {available_cities[:10]}\")\n",
        "        \n",
        "        # Check target cities\n",
        "        target_cities_found = [city for city in target_cities if city in available_cities]\n",
        "        print(f\"\\nüéØ Target Cities Found ({len(target_cities_found)}/{len(target_cities)}):\")\n",
        "        for city in target_cities:\n",
        "            if city in available_cities:\n",
        "                count = len(df_raw[df_raw['City'] == city])\n",
        "                print(f\"  ‚úÖ {city}: {count:,} records\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {city}: Not found\")\n",
        "        \n",
        "        # Check for similar city names\n",
        "        if len(target_cities_found) < len(target_cities):\n",
        "            print(f\"\\nüîç Checking for similar city names...\")\n",
        "            for target in target_cities:\n",
        "                if target not in available_cities:\n",
        "                    similar = [city for city in available_cities if target.lower() in city.lower() or city.lower() in target.lower()]\n",
        "                    if similar:\n",
        "                        print(f\"  üí° {target} ‚Üí Similar: {similar}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è 'City' column not found. Available columns: {list(df_raw.columns)}\")\n",
        "    \n",
        "    # Check AQI column\n",
        "    if 'AQI' in df_raw.columns:\n",
        "        print(f\"\\nüå¨Ô∏è AQI Analysis:\")\n",
        "        print(f\"  Range: {df_raw['AQI'].min():.2f} to {df_raw['AQI'].max():.2f}\")\n",
        "        print(f\"  Mean: {df_raw['AQI'].mean():.2f}\")\n",
        "        print(f\"  Median: {df_raw['AQI'].median():.2f}\")\n",
        "        print(f\"  Missing AQI values: {df_raw['AQI'].isnull().sum():,} ({df_raw['AQI'].isnull().sum()/len(df_raw)*100:.2f}%)\")\n",
        "        \n",
        "        # AQI distribution\n",
        "        print(f\"\\nüìä AQI Distribution:\")\n",
        "        aqi_stats = df_raw['AQI'].describe()\n",
        "        print(aqi_stats)\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è 'AQI' column not found. Available columns: {list(df_raw.columns)}\")\n",
        "    \n",
        "    # Check date column\n",
        "    if 'Date' in df_raw.columns:\n",
        "        df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
        "        print(f\"\\nüìÖ Date Range:\")\n",
        "        print(f\"  From: {df_raw['Date'].min()}\")\n",
        "        print(f\"  To: {df_raw['Date'].max()}\")\n",
        "        print(f\"  Duration: {(df_raw['Date'].max() - df_raw['Date'].min()).days} days\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è 'Date' column not found. Available columns: {list(df_raw.columns)}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No data loaded. Please check file paths and formats.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèôÔ∏è FILTERING FOR TARGET CITIES\n",
            "==================================================\n",
            "Available cities: 26\n",
            "  ‚úÖ Delhi: 2,009 records\n",
            "  ‚úÖ Kolkata: 814 records\n",
            "  ‚úÖ Hyderabad: 2,006 records\n",
            "  ‚úÖ Chennai: 2,009 records\n",
            "  ‚úÖ Visakhapatnam: 1,462 records\n",
            "\n",
            "üìä Filtered dataset shape: (8300, 16)\n",
            "   Original: (29531, 16)\n",
            "   Filtered: (8300, 16)\n",
            "\n",
            "üéØ Target cities data summary:\n",
            "City\n",
            "Delhi            2009\n",
            "Chennai          2009\n",
            "Hyderabad        2006\n",
            "Visakhapatnam    1462\n",
            "Kolkata           814\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Filter data for target cities\n",
        "print(\"üèôÔ∏è FILTERING FOR TARGET CITIES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "target_cities = ['Delhi', 'Bangalore', 'Kolkata', 'Hyderabad', 'Chennai', 'Visakhapatnam']\n",
        "\n",
        "if not df_raw.empty and 'City' in df_raw.columns:\n",
        "    # Check available cities and find matches\n",
        "    available_cities = df_raw['City'].unique()\n",
        "    print(f\"Available cities: {len(available_cities)}\")\n",
        "    \n",
        "    # Find exact matches and similar names\n",
        "    city_mapping = {}\n",
        "    for target in target_cities:\n",
        "        if target in available_cities:\n",
        "            city_mapping[target] = target\n",
        "        else:\n",
        "            # Look for similar names (case-insensitive)\n",
        "            similar = [city for city in available_cities \n",
        "                      if target.lower() in city.lower() or city.lower() in target.lower()]\n",
        "            if similar:\n",
        "                city_mapping[target] = similar[0]  # Take first match\n",
        "                print(f\"  üí° Mapping: {target} ‚Üí {similar[0]}\")\n",
        "    \n",
        "    # Filter data for target cities\n",
        "    target_city_data = []\n",
        "    for target, actual_city in city_mapping.items():\n",
        "        city_data = df_raw[df_raw['City'] == actual_city].copy()\n",
        "        city_data['City'] = target  # Standardize city name\n",
        "        target_city_data.append(city_data)\n",
        "        print(f\"  ‚úÖ {target}: {len(city_data):,} records\")\n",
        "    \n",
        "    if target_city_data:\n",
        "        df_filtered = pd.concat(target_city_data, ignore_index=True)\n",
        "        print(f\"\\nüìä Filtered dataset shape: {df_filtered.shape}\")\n",
        "        print(f\"   Original: {df_raw.shape}\")\n",
        "        print(f\"   Filtered: {df_filtered.shape}\")\n",
        "    else:\n",
        "        print(\"‚ùå No target cities found in dataset\")\n",
        "        df_filtered = df_raw.copy()\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot filter cities - City column not found\")\n",
        "    df_filtered = df_raw.copy()\n",
        "\n",
        "print(f\"\\nüéØ Target cities data summary:\")\n",
        "if 'City' in df_filtered.columns:\n",
        "    city_counts = df_filtered['City'].value_counts()\n",
        "    print(city_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßπ HANDLING MISSING VALUES\n",
            "==================================================\n",
            "Initial dataset shape: (8300, 16)\n",
            "‚úÖ Removed 612 rows with null AQI\n",
            "   Remaining rows: 7,688\n",
            "\n",
            "üìä Missing values after AQI cleaning:\n",
            "Columns with missing values:\n",
            "  PM2.5: 20 (0.26%)\n",
            "  PM10: 1,909 (24.83%)\n",
            "  NO: 18 (0.23%)\n",
            "  NO2: 25 (0.33%)\n",
            "  NOx: 15 (0.20%)\n",
            "  NH3: 697 (9.07%)\n",
            "  CO: 13 (0.17%)\n",
            "  SO2: 124 (1.61%)\n",
            "  O3: 103 (1.34%)\n",
            "  Benzene: 329 (4.28%)\n",
            "  Toluene: 303 (3.94%)\n",
            "  Xylene: 3,140 (40.84%)\n",
            "\n",
            "üìä Final cleaned dataset shape: (7688, 16)\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Handle missing values - Remove rows with null AQI\n",
        "print(\"\\nüßπ HANDLING MISSING VALUES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_filtered.empty and 'AQI' in df_filtered.columns:\n",
        "    initial_shape = df_filtered.shape\n",
        "    print(f\"Initial dataset shape: {initial_shape}\")\n",
        "    \n",
        "    # Remove rows with null AQI (target variable)\n",
        "    df_no_null_aqi = df_filtered.dropna(subset=['AQI'])\n",
        "    null_aqi_removed = initial_shape[0] - df_no_null_aqi.shape[0]\n",
        "    \n",
        "    print(f\"‚úÖ Removed {null_aqi_removed:,} rows with null AQI\")\n",
        "    print(f\"   Remaining rows: {df_no_null_aqi.shape[0]:,}\")\n",
        "    \n",
        "    # Check other missing values\n",
        "    print(f\"\\nüìä Missing values after AQI cleaning:\")\n",
        "    missing_after = df_no_null_aqi.isnull().sum()\n",
        "    missing_after = missing_after[missing_after > 0]\n",
        "    \n",
        "    if len(missing_after) > 0:\n",
        "        print(\"Columns with missing values:\")\n",
        "        for col, count in missing_after.items():\n",
        "            percentage = (count / len(df_no_null_aqi)) * 100\n",
        "            print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
        "    else:\n",
        "        print(\"‚úÖ No missing values remaining!\")\n",
        "    \n",
        "    df_cleaned = df_no_null_aqi.copy()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot clean missing values - AQI column not found\")\n",
        "    df_cleaned = df_filtered.copy()\n",
        "\n",
        "print(f\"\\nüìä Final cleaned dataset shape: {df_cleaned.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç OUTLIER DETECTION AND TREATMENT\n",
            "==================================================\n",
            "Analyzing outliers in 13 numeric columns:\n",
            "Columns: ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']\n",
            "  PM2.5: 680 outliers (8.84%)\n",
            "  PM10: 345 outliers (4.49%)\n",
            "  NO: 877 outliers (11.41%)\n",
            "  NO2: 201 outliers (2.61%)\n",
            "  NOx: 695 outliers (9.04%)\n",
            "  NH3: 424 outliers (5.52%)\n",
            "  CO: 509 outliers (6.62%)\n",
            "  SO2: 329 outliers (4.28%)\n",
            "  O3: 335 outliers (4.36%)\n",
            "  Benzene: 418 outliers (5.44%)\n",
            "  Toluene: 553 outliers (7.19%)\n",
            "  Xylene: 248 outliers (3.23%)\n",
            "  AQI: 569 outliers (7.40%)\n",
            "\n",
            "üìä Outlier Treatment Summary:\n",
            "  PM2.5: 680 outliers (bounds: -35.98 to 145.92)\n",
            "  PM10: 345 outliers (bounds: -96.72 to 352.20)\n",
            "  NO: 877 outliers (bounds: -14.41 to 39.84)\n",
            "  NO2: 201 outliers (bounds: -26.37 to 88.47)\n",
            "  NOx: 695 outliers (bounds: -22.71 to 79.33)\n",
            "  NH3: 424 outliers (bounds: -30.83 to 84.03)\n",
            "  CO: 509 outliers (bounds: -0.46 to 2.18)\n",
            "  SO2: 329 outliers (bounds: -6.64 to 26.53)\n",
            "  O3: 335 outliers (bounds: -11.43 to 82.62)\n",
            "  Benzene: 418 outliers (bounds: -5.22 to 9.98)\n",
            "  Toluene: 553 outliers (bounds: -14.32 to 29.79)\n",
            "  Xylene: 248 outliers (bounds: -3.45 to 6.64)\n",
            "  AQI: 569 outliers (bounds: -73.50 to 346.50)\n",
            "\n",
            "‚úÖ Treated 6183 outlier values using capping method\n",
            "\n",
            "üìä Final processed dataset shape: (7688, 16)\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Outlier Detection and Treatment\n",
        "print(\"\\nüîç OUTLIER DETECTION AND TREATMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_cleaned.empty:\n",
        "    # Select numeric columns for outlier analysis\n",
        "    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    # Remove Date columns if present\n",
        "    numeric_cols = [col for col in numeric_cols if 'Date' not in col and col not in ['year', 'month', 'day']]\n",
        "    \n",
        "    print(f\"Analyzing outliers in {len(numeric_cols)} numeric columns:\")\n",
        "    print(f\"Columns: {numeric_cols}\")\n",
        "    \n",
        "    # Detect outliers using IQR method\n",
        "    outlier_summary = {}\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in df_cleaned.columns:\n",
        "            Q1 = df_cleaned[col].quantile(0.25)\n",
        "            Q3 = df_cleaned[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            \n",
        "            outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)]\n",
        "            outlier_count = len(outliers)\n",
        "            outlier_percentage = (outlier_count / len(df_cleaned)) * 100\n",
        "            \n",
        "            outlier_summary[col] = {\n",
        "                'count': outlier_count,\n",
        "                'percentage': outlier_percentage,\n",
        "                'lower_bound': lower_bound,\n",
        "                'upper_bound': upper_bound\n",
        "            }\n",
        "            \n",
        "            print(f\"  {col}: {outlier_count:,} outliers ({outlier_percentage:.2f}%)\")\n",
        "    \n",
        "    # Show outlier treatment options\n",
        "    print(f\"\\nüìä Outlier Treatment Summary:\")\n",
        "    for col, info in outlier_summary.items():\n",
        "        if info['count'] > 0:\n",
        "            print(f\"  {col}: {info['count']} outliers (bounds: {info['lower_bound']:.2f} to {info['upper_bound']:.2f})\")\n",
        "    \n",
        "    # Apply outlier treatment (capping method)\n",
        "    df_treated = df_cleaned.copy()\n",
        "    outliers_treated = 0\n",
        "    \n",
        "    for col, info in outlier_summary.items():\n",
        "        if info['count'] > 0:\n",
        "            # Cap outliers instead of removing them\n",
        "            df_treated[col] = np.where(df_treated[col] < info['lower_bound'], info['lower_bound'], df_treated[col])\n",
        "            df_treated[col] = np.where(df_treated[col] > info['upper_bound'], info['upper_bound'], df_treated[col])\n",
        "            outliers_treated += info['count']\n",
        "    \n",
        "    if outliers_treated > 0:\n",
        "        print(f\"\\n‚úÖ Treated {outliers_treated} outlier values using capping method\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ No outliers found in the dataset\")\n",
        "    \n",
        "    df_final = df_treated.copy()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for outlier analysis\")\n",
        "    df_final = df_cleaned.copy()\n",
        "\n",
        "print(f\"\\nüìä Final processed dataset shape: {df_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ SAVING PROCESSED DATA\n",
            "==================================================\n",
            "‚úÖ Processed data saved to: ../data/processed/cleaned_data.csv\n",
            "   Shape: (7688, 16)\n",
            "   File size: 0.76 MB\n",
            "\n",
            "üìä PREPROCESSING SUMMARY\n",
            "==================================================\n",
            "Original dataset: (29531, 16)\n",
            "After city filtering: (8300, 16)\n",
            "After missing value removal: (7688, 16)\n",
            "Final processed dataset: (7688, 16)\n",
            "\n",
            "üèôÔ∏è City-wise record counts:\n",
            "  Delhi: 1,999 records\n",
            "  Chennai: 1,884 records\n",
            "  Hyderabad: 1,880 records\n",
            "  Visakhapatnam: 1,171 records\n",
            "  Kolkata: 754 records\n",
            "\n",
            "üå¨Ô∏è Final AQI Statistics:\n",
            "  Range: 22.00 to 346.50\n",
            "  Mean: 148.85\n",
            "  Std: 89.39\n",
            "\n",
            "üéâ Preprocessing completed successfully!\n",
            "Next step: Run feature engineering notebook (03_feature_engineering.ipynb)\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Save processed data\n",
        "print(\"\\nüíæ SAVING PROCESSED DATA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not df_final.empty:\n",
        "    # Create output directory if it doesn't exist\n",
        "    output_dir = \"../data/processed/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save the processed dataset\n",
        "    output_file = output_dir + \"cleaned_data.csv\"\n",
        "    df_final.to_csv(output_file, index=False)\n",
        "    \n",
        "    print(f\"‚úÖ Processed data saved to: {output_file}\")\n",
        "    print(f\"   Shape: {df_final.shape}\")\n",
        "    print(f\"   File size: {os.path.getsize(output_file) / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Display final summary\n",
        "    print(f\"\\nüìä PREPROCESSING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Original dataset: {df_raw.shape if not df_raw.empty else 'N/A'}\")\n",
        "    print(f\"After city filtering: {df_filtered.shape if not df_filtered.empty else 'N/A'}\")\n",
        "    print(f\"After missing value removal: {df_cleaned.shape if not df_cleaned.empty else 'N/A'}\")\n",
        "    print(f\"Final processed dataset: {df_final.shape}\")\n",
        "    \n",
        "    # City-wise summary\n",
        "    if 'City' in df_final.columns:\n",
        "        print(f\"\\nüèôÔ∏è City-wise record counts:\")\n",
        "        city_counts = df_final['City'].value_counts()\n",
        "        for city, count in city_counts.items():\n",
        "            print(f\"  {city}: {count:,} records\")\n",
        "    \n",
        "    # AQI summary\n",
        "    if 'AQI' in df_final.columns:\n",
        "        print(f\"\\nüå¨Ô∏è Final AQI Statistics:\")\n",
        "        print(f\"  Range: {df_final['AQI'].min():.2f} to {df_final['AQI'].max():.2f}\")\n",
        "        print(f\"  Mean: {df_final['AQI'].mean():.2f}\")\n",
        "        print(f\"  Std: {df_final['AQI'].std():.2f}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No processed data to save\")\n",
        "\n",
        "print(f\"\\nüéâ Preprocessing completed successfully!\")\n",
        "print(f\"Next step: Run feature engineering notebook (03_feature_engineering.ipynb)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PredictiveAnalysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
